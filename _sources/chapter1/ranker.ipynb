{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbef708e",
   "metadata": {},
   "source": [
    "(chapter1_part6)=\n",
    "\n",
    "# Ranking Problem\n",
    "The ranking problem is a type of supervised or semi-supervised learning problem where the goal\n",
    "is to predict the relative order of a set of items. It is commonly used in search engines,\n",
    "recommender systems, and other applications where the order of the response is vital. Ranking\n",
    "models usually try to predict a relevance score and sorting is made based on these scores.\n",
    "Strictly speaking, $s = f(x)$, $s$ - stands for ranking model and for each\n",
    "input $x = (q, d)$ where $q$ is a query and $d$ is a document we predict *relevance scores*\n",
    "\n",
    "Depending on the context and business needs, the prediction of the relevance score can be considered as:\n",
    "- `Binary classification` - whether the user will click on the recommended movie;\n",
    "- `Regression` - prediction of watch time by a user for the particularly recommended movie;\n",
    "- `Learning-to-rank`\n",
    "\n",
    "Considering that the first two tasks are quite widespread, here we will focus in learning-to-rank\n",
    "class of models. So, why do we have a distinct class of learning-to-rank models while it is still?\n",
    "The main difference between learning to rank and classification/regression models is that\n",
    "classification/regression models predict a label or value for a single input\n",
    "while learning to rank models predict a ranking for a list of inputs. You have a\n",
    "list of items and you can make pair comparisons within this set and decide the order within that\n",
    "set. In the case of regression/classification, you will not be able to do that.\n",
    "\n",
    "![](img/ranking_example_1.png)\n",
    "\n",
    "# Training learnig-to-rank models\n",
    "In the learning-to-rank pipeline there are three approaches to training the model:\n",
    "- `Pointwise`: basically, it takes a single item from a list and computes loss using\n",
    "only its information. It resembles the same as if a classifier/regressor is trained;\n",
    "Some examples: cosine distance between embeddings, logistic regression based on some \n",
    "features with binary relevance target etc, BM25 (it is an \"advanced\" version of tf-idf).\n",
    "\n",
    "- `Pairwise`: in this approach, a pair of documents is used to minimize the loss.\n",
    "The idea is to minimize the number of swaps in the final ordered list. Some of the most\n",
    "popular methods are [RankNet, LambdaRank, and LambdaMART](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf);\n",
    "\n",
    "In this approach, there are three main methods to train the model - `RankNet`, `LambdaRank` & `LambdaMART`:\n",
    "\n",
    "**RankNet**\n",
    "\n",
    "This cost function tries to minimize the number of swaps in the final ordered list. Originally,\n",
    "it was developed for optimization parameters in neural nets, but in general the underlying model\n",
    "can be set to any. The formula:\n",
    "\n",
    "$P_{ij} \\equiv P(U_{i}>U_{j}) \\equiv \\frac{1}{1 + \\exp^{-\\sigma(s_{i} - s{j})}}$\n",
    "\n",
    "Instead of directly predicting the score of each item one by one, RankNet proposed\n",
    "to model the target probabilities between any two items. For instance, \n",
    "take 3 movies: `movie 1`, `movie 2`, and `movie 3` with scores 0, 5, and 3 respectively.\n",
    "The 3 distinct combinations give us `P(movie 1 & movie 2) = 0`, `P(movie 1 & movie 3) = 0`\n",
    "and `P(movie 2 & movie 3) = 1` - this is considered as one training sample and the model tries\n",
    "to model this distribution. \n",
    "\n",
    "**LambdaRank**\n",
    "\n",
    "It was derived after `RankNet`, researchers found that during the process you do not need values of costs,\n",
    "but only the gradients of the cost w.r.t model score. The formula that explains the idea \n",
    "with NDCG is the following:\n",
    "\n",
    "$\\lambda_{ij} = \\frac{\\partial C(s_{i} - s_{j})}{\\partial s_{i}} = \\frac{-\\alpha}{1 + \\exp^{\\alpha(s_{i} - s_{j})}}|\\Delta NDCG|$,\n",
    "where C is the cost function, $\\delta NDCG$ stands for how much NDCG will change if swap *i* and *j*\n",
    "\n",
    "**LambdaMART**\n",
    "\n",
    "This approach combines `LambdaRank` &  `Multiple Additive Regression Trees (MART)`. The approach is\n",
    "quite straightforward: we use gradient-boosted trees for prediction tasks and incorporate `LambdaRank`\n",
    "cost function into the model to convert it to solve a ranking problem. On experimental data, this\n",
    "method outperformed `RankNet` & `LambdaRank`. However, both the Pointwise and Pairwise methods\n",
    "fail to determine the impact of the whole group (list of items). Here comes the next method\n",
    "\n",
    "- `Listwise`: as in the naming, it takes the whole list of candidates at once and tries to rank\n",
    "documents optimally. For that it uses two approaches:\n",
    "1. Direct optimization of information retrieval (IR) metric such as NDCG via approximation\n",
    "with [SoftRank](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/SoftRankWsdm08Submitted.pdf) / [AdaRank](https://www.semanticscholar.org/paper/AdaRank%3A-a-boosting-algorithm-for-information-Xu-Li/a489d95fb930401c1f4b7d92bb139d271d49abbf);\n",
    "2. Minimization of the loss that is defined based on your domain knowledge of what you are trying to achieve.\n",
    "These are ListNet and ListMLE losses.\n",
    "\n",
    "\n",
    "There are two methods in the listwise approach - `ListNet` & `ListMLE`. These are the two most popular ones.\n",
    "In this task, each item has a set of features that describe its properties and relevance to the query.\n",
    "The goal is to learn a ranking function that maps these features to a ranking order.\n",
    "\n",
    "**ListNet**\n",
    "\n",
    "It is a listwise version of the aforementioned `RankNet`. It uses a cross-entropy loss function along\n",
    "with gradient descent to optimize parameters for the neural net. In cases when we have only two items\n",
    "in a list the result of the `ListNet` coincides with `RankNet`. The idea behind ListNet is to learn\n",
    "a probability distribution over the permutations of the items in the list, such that the more\n",
    "relevant items appear at the top of the list with higher probability. The training objective \n",
    "of ListNet is to maximize the likelihood of the ground-truth permutation, which is the permutation\n",
    "that corresponds to the actual relevance of the items. This is done by minimizing the cross-entropy\n",
    "loss between the predicted and ground-truth probability distributions.\n",
    "So, how do we get probabilities in the first place, and how permutations are involved? Let's go step-by-step.\n",
    "\n",
    "Let's assume, we have three movies to rank `The Godfather, `Avatar`, & `Ozark`\n",
    "What are all possible permutations for these three objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be38e42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Godfather', 'Ozark', 'Avatar')\n",
      "('The Godfather', 'Avatar', 'Ozark')\n",
      "('Ozark', 'The Godfather', 'Avatar')\n",
      "('Ozark', 'Avatar', 'The Godfather')\n",
      "('Avatar', 'The Godfather', 'Ozark')\n",
      "('Avatar', 'Ozark', 'The Godfather')\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "movies_to_rank = {'The Godfather', 'Avatar', 'Ozark'}\n",
    "permutations_list = list(permutations(movies_to_rank))\n",
    "\n",
    "for i in permutations_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff262df",
   "metadata": {},
   "source": [
    "Now, we have all possible permutations for this object, and, say, we predicted some scores that can be\n",
    "used to rank these objects. Next, we are interested in how we can use these scores to determine\n",
    "which permutation of all combinations is more relevant than the other. Here is how it proposed in the\n",
    "[paper](http://times.cs.uiuc.edu/course/598f14/l2r.pdf)\n",
    "\n",
    "$P_{s}(\\pi) = \\prod^n_{j = 1} \\frac {\\phi(s_{\\pi(j)})} {\\sum^n_{k = j} \\phi(s_{\\pi(k)})}$\n",
    "\n",
    "Wow, that's a big formula. Let's break it down:\n",
    "- We calculate the probability for a particular permutation $\\pi$ given some predicted scores $s$;\n",
    "- Big $\\prod$ stands for calculation of the product of $n$ terms (permutations);\n",
    "- Next, $\\phi$ is a transformation function with a required property - `an increasing and strictly positive function`\n",
    "i.e. the higher predicted score $s$ the higher value of the function\n",
    "\n",
    "Considering our example, we can continue by generating some random scores as our predictions and pick one\n",
    "of the permutations randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8497ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The Godfather': 0.4967141530112327, 'Ozark': -0.13826430117118466, 'Avatar': 0.6476885381006925}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "scores_dict = {x: np.random.randn(1)[0] for x in movies_to_rank}  \n",
    "print(scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9f0d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ozark', 'Avatar', 'The Godfather')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "pi = random.choice(permutations_list)\n",
    "print(pi)\n",
    "\n",
    "# unpack pi and assign movies to scores\n",
    "score_movie_pos_1, score_movie_pos_2, score_movie_pos_3 = scores_dict[pi[0]], scores_dict[pi[1]], scores_dict[pi[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968b1a9",
   "metadata": {},
   "source": [
    "Now, we can move to use our aforementioned formula using these numbers. There are going to be three\n",
    "terms with combinrations: The Godfather vs The Godfather  + Avatar + Ozark, Avatar vs Avatar + Ozark\n",
    "and finally Ozark vs Ozark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e137cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First term is: 0.19679312100689936\n",
      "Second term is: 0.5376720676558362\n",
      "Third term is: 1.0\n"
     ]
    }
   ],
   "source": [
    "first_term = np.exp(score_movie_pos_1) / (np.exp(score_movie_pos_1) + np.exp(score_movie_pos_2)\\\n",
    "                                         + np.exp(score_movie_pos_3))\n",
    "\n",
    "second_term = np.exp(score_movie_pos_2) / (np.exp(score_movie_pos_2) + np.exp(score_movie_pos_3))\n",
    "\n",
    "third_term = np.exp(score_movie_pos_3) / np.exp(score_movie_pos_3)\n",
    "\n",
    "print(f'First term is: {first_term}')\n",
    "print(f'Second term is: {second_term}')\n",
    "print(f'Third term is: {third_term}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6b3a1",
   "metadata": {},
   "source": [
    "What we have done so far? $P_{s}(<The Godfather, Avatar, Ozark>) = \\prod^3_{j = 1} \\frac {\\phi(s_{\\pi(j)})} {\\sum^3_{k = j} \\phi(s_{\\pi(k)})}$ which is equal to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9548702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation probability is: 0.10581016427222475\n"
     ]
    }
   ],
   "source": [
    "permutation_proba = first_term * second_term * third_term\n",
    "\n",
    "print(f'Permutation probability is: {permutation_proba}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf2d4f",
   "metadata": {},
   "source": [
    "In this manner, we can calculate the permutation probability for each set in our list of all permutations.\n",
    "Thus, the final probabilities will sum up to 1 and we can sort using this probability! Now,\n",
    "we know how to get probability distributions of permutations. So far, we used some random numbers\n",
    "to calculate this, but how do we get these scores in real life? To get scores\n",
    "The neural network model is used. We define any architecture and feed `query - document` features\n",
    "and use cross-entropy / KL-divergence to train the model. Thanks to this [source](https://embracingtherandom.com/machine-learning/tensorflow/ranking/deep-learning/learning-to-rank-part-2/), all further details can be found there.\n",
    "Finally, it is worth mentioning that calculating all permutations is too computationally expensive\n",
    "and authors propose the `Top One Probability` method to avoid the calculation of all permutations.\n",
    "\n",
    "\n",
    "**ListMLE**\n",
    "\n",
    "ListMLE is another learning-to-rank method that uses a maximum likelihood estimation (MLE)\n",
    "framework to learn the ranking function. The idea behind ListMLE is to directly model the\n",
    "likelihood of the ground-truth permutation, rather than the probability distribution.\n",
    "The training objective of ListMLE is to maximize the log-likelihood of the ground-truth\n",
    "permutation under the Mallows model. This is done by minimizing the negative log-likelihood,\n",
    "which is equivalent to minimizing the Kendall tau distance between the predicted\n",
    "and ground-truth permutations. \n",
    "\n",
    "A more detailed explanation of these three approaches in learning-to-rank tasks are explained\n",
    "[here](http://icml2008.cs.helsinki.fi/papers/167.pdf).\n",
    "\n",
    "\n",
    "# Source & further recommendations\n",
    "- [Catboost Documentation](https://catboost.ai/)\n",
    "- [Learning-to-rank explained](https://embracingtherandom.com/machine-learning/tensorflow/ranking/deep-learning/learning-to-rank-part-2/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   11,
   110,
   118,
   135,
   143,
   151,
   156,
   167,
   170,
   174
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}